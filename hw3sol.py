# -*- coding: utf-8 -*-
"""HW3SOL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bYbk8b1NFUkKKI_OHaXhCj5jiDg5Yz6l

Sample Solution
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/Dataset (3).zip'

"""Design a dense neural network to classify your dataset into different classes. Your network should have the following specifications:

a) Input layer with an appropriate input size for the images.

b) At least two hidden layers with a sufficient number of neurons.

c) An output layer with the appropriate number of units for the classification task.

d) Use the ReLU activation function for the hidden layers and an appropriate activation function for the output layer.

e) Choose an appropriate loss function and optimization algorithm.


"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Dropout
from keras import models, layers
from keras.layers import Conv1D
from keras.layers import Input
from keras.layers import BatchNormalization, ReLU, Add
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

"""read the dataset:"""

import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.keras.utils import to_categorical

# Read and label the images
X = []
Y = []

for label, directory in enumerate(['/content/Cookie', '/content/Dimsum', '/content/Sushi']):
    for i in os.listdir(directory):
        if i.endswith(".jpg") or i.endswith(".png"):
            image = cv2.imread(os.path.join(directory, i))
            image = cv2.resize(image, (100, 100))  # Adjust target width and height
            X.append(image)
            Y.append(label)

# Convert lists to numpy arrays
X = np.array(X)
Y = np.array(Y)

# Shuffle the data
X, Y = shuffle(X, Y, random_state=42)

# Convert labels to one-hot encoding
Y_one_hot = to_categorical(Y, num_classes=3)

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(X, Y_one_hot, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

def create_dense_model():
  model = Sequential([
                    Flatten(input_shape = (width, height,3)),
                    Dense(units=500, activation= 'relu'),
                    Dense(units=200, activation= 'relu'),
                    Dense(units=100, activation= 'relu'),
                    Dense(units=100, activation= 'relu'),
                    Dense(units=100, activation= 'relu'),
                    Dense(3, activation = 'softmax' ),])
  return model

model = create_dense_model()
model.compile(loss='categorical_crossentropy',optimizer= Adam(learning_rate=0.001) ,metrics=['accuracy'])

history = model.fit(X_train, y_train,validation_data=(X_val, y_val), epochs=20, verbose=1, batch_size =32)

training_loss = history.history['loss']
validation_loss = history.history['val_loss']

import matplotlib.pyplot as plt

# Access loss history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']

# Plot the losses
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.title('Training and Validation Losses Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import classification_report

# predictions
y_pred = model.predict(X_test)

# Convert one-hot encoded labels back to integer labels for classification_report
y_test_argmax = np.argmax(y_test, axis=1)
y_pred_argmax = np.argmax(y_pred, axis=1)

# Generate classification report
report = classification_report(y_test_argmax, y_pred_argmax)
print(report)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize

# predictions
y_pred = model.predict(X_test)


# Initialize variables for ROC curve
fpr = dict()
tpr = dict()
roc_auc = dict()

# Compute ROC curve and ROC area for each class
for i in range(3):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(10, 7))
colors = ['red', 'green', 'blue']  # Add more colors as needed
for i, color in zip(range(y_test.shape[1]), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (class {i}), AUC = {roc_auc[i]:.2f}')

plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Multi-Class')
plt.legend(loc="lower right")
plt.show()

"""-	Perform k-fold cross-validation (k = 5) and report the average and standard deviation of the metrics across all folds."""

from sklearn.model_selection import KFold

# Define the number of folds
num_folds = 5  # Adjust as needed

# Initialize KFold
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Lists to store evaluation metrics for each fold
acc_per_fold = []
loss_per_fold = []

# Iterate through the folds
for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):
    print(f"Training on fold {fold + 1}...")

    # Create a new instance of the model for each fold
    model = create_dense_model()

    # Extract the training and validation sets for this fold
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model on the current fold
    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=3)

    # Evaluate the model
    scores = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    acc_per_fold.append(scores[1])
    loss_per_fold.append(scores[0])


# Print average and standard deviation of performance across all folds
average_accuracy = np.mean(acc_per_fold) * 100
std_dev_accuracy = np.std(acc_per_fold) * 100

average_loss = np.mean(loss_per_fold)
std_dev_loss = np.std(loss_per_fold)

print("\nAverage Accuracy: {:.2f}% (+/- {:.2f}%)".format(average_accuracy, std_dev_accuracy))
print("Average Loss: {:.4f} (+/- {:.4f})".format(average_loss, std_dev_loss))

"""Design a Convolutional Neural Network (CNN) to classify the images in your dataset into their respective classes. Your CNN should have the following architecture:

a) Input layer for the images.

b) A sequence of Convolutional layers with appropriate kernel sizes, strides, and activation functions. You can also include pooling layers if needed.

c) At least two fully connected (dense) layers with an appropriate number of neurons.

d) Use the appropriate activation function for the fully connected layers.

e) Choose an appropriate loss function and optimization algorithm.

"""

from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

def build_cnn(input_shape, num_classes):
    input_layer = Input(shape=input_shape)

    # First convolutional layer with max pooling
    c = Conv2D(8, kernel_size=(3, 3), activation='sigmoid', padding="same")(input_layer)
    c = MaxPooling2D(pool_size=(2, 2))(c)

    # Second convolutional layer with max pooling
    c = Conv2D(16, kernel_size=(3, 3), activation='sigmoid', padding="same")(c)
    c = MaxPooling2D(pool_size=(2, 2))(c)

    # Third convolutional layer with max pooling
    c = Conv2D(32, kernel_size=(3, 3), activation='sigmoid', padding="same")(c)
    c = MaxPooling2D(pool_size=(2, 2))(c)

    # Fourth convolutional layer with max pooling
    c = Conv2D(64, kernel_size=(3, 3), activation='sigmoid', padding="same")(c)
    c = MaxPooling2D(pool_size=(2, 2))(c)

    # Fifth convolutional layer with max pooling
    c = Conv2D(128, kernel_size=(3, 3), activation='sigmoid', padding="same")(c)
    c = MaxPooling2D(pool_size=(2, 2))(c)

    flatten = Flatten()(c)
    dense1 = Dense(32, activation='relu')(flatten)
    output = Dense(num_classes, activation='softmax')(dense1)

    model = Model(inputs=input_layer, outputs=output)
    optimizer = Adam(learning_rate=0.001)
    loss = CategoricalCrossentropy()
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    return model

# Assuming input_shape is the shape of your input data (e.g., (width, height, channels))
input_shape = (100, 100, 3)
num_classes = 3  # Change to the actual number of classes
model = build_cnn((100,100,3),3)
history = model.fit(X_train, y_train,validation_data=(X_val, y_val), epochs=20, verbose=1, batch_size =32)

training_loss = history.history['loss']
validation_loss = history.history['val_loss']

import matplotlib.pyplot as plt

# Access loss history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']

# Plot the losses
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.title('Training and Validation Losses Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import classification_report

# pred
y_pred = model.predict(X_test)

# Convert one-hot encoded labels back to integer labels for classification_report
y_test_argmax = np.argmax(y_test, axis=1)
y_pred_argmax = np.argmax(y_pred, axis=1)

# Generate classification report
report = classification_report(y_test_argmax, y_pred_argmax)
print(report)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize

# Assuming you have model predictions (y_pred) on the test set
y_pred = model.predict(X_test)


# Initialize variables for ROC curve
fpr = dict()
tpr = dict()
roc_auc = dict()

# Compute ROC curve and ROC area for each class
for i in range(3):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(10, 7))
colors = ['red', 'green', 'blue']  # Add more colors as needed
for i, color in zip(range(y_test.shape[1]), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (class {i}), AUC = {roc_auc[i]:.2f}')

plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Multi-Class')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import KFold

# Define the number of folds
num_folds = 5  # Adjust as needed

# Initialize KFold
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Lists to store evaluation metrics for each fold
acc_per_fold = []
loss_per_fold = []

# Iterate through the folds
for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):
    print(f"Training on fold {fold + 1}...")

    # Create a new instance of the model for each fold
    model = build_cnn((100,100,3),3)

    # Extract the training and validation sets for this fold
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model on the current fold
    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=3)

    # Evaluate
    scores = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    acc_per_fold.append(scores[1])
    loss_per_fold.append(scores[0])


# Print average and standard deviation of performance across all folds
average_accuracy = np.mean(acc_per_fold) * 100
std_dev_accuracy = np.std(acc_per_fold) * 100

average_loss = np.mean(loss_per_fold)
std_dev_loss = np.std(loss_per_fold)

print("\nAverage Accuracy: {:.2f}% (+/- {:.2f}%)".format(average_accuracy, std_dev_accuracy))
print("Average Loss: {:.4f} (+/- {:.4f})".format(average_loss, std_dev_loss))

"""Question 3: Using ImageNet Pretrained CNNs

Instead of designing a CNN from scratch, you want to leverage the knowledge from large-scale datasets like ImageNet. Choose a pre-trained CNN and follow these steps:

a) Load the pre-trained CNN without the top classification layer.
b) Add a new classification head on top of the pre-trained network to match the number of classes in your dataset.
c) Freeze the weights of the pre-trained layers and only train the newly added classification head.

-	Train your neural network using a 60-20-20 split on your chosen dataset and report the following metrics:
•	Accuracy: percentage of correctly classified images.
•	Loss: the value of the loss function after training.
•	Precision, Recall, and F1-score for each class.
•	ROC curve and AUC.

-	Perform k-fold cross-validation (k = 5) and report the average and standard deviation of the metrics across all folds.

"""

from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC, Precision, Recall
from sklearn.metrics import classification_report, roc_curve, auc
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Load the pre-trained VGG16 model without the top classification layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))

# Freeze the weights of the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add a new classification head on top of the pre-trained network
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=x)

# Compile the model
model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy', AUC(), Precision(), Recall()])


# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model on the test set
test_metrics = model.evaluate(X_test, y_test)

# Print accuracy and loss
print(f'Test Accuracy: {test_metrics[1]:.4f}')
print(f'Test Loss: {test_metrics[0]:.4f}')

# Make predictions on the test set
y_pred = model.predict(X_test)

# Convert one-hot encoded labels back to integers for classification report
y_test_argmax = np.argmax(y_test, axis=1)
y_pred_argmax = np.argmax(y_pred, axis=1)

# Print classification report
print(classification_report(y_test_argmax, y_pred_argmax))

# Plot ROC curve and calculate AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(10, 7))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Each Class')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import clone_model

# Define the number of folds
num_folds = 5  # Adjust as needed

# Initialize KFold
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Lists to store evaluation metrics for each fold
acc_per_fold = []
loss_per_fold = []

# Load the pre-trained VGG16 model without the top classification layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))

# Freeze the weights of the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add a new classification head on top of the pre-trained network
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dense(num_classes, activation='softmax')(x)

pretrained_model = Model(inputs=base_model.input, outputs=x)

# Compile the pre-trained model
pretrained_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Iterate through the folds
for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):
    print(f"Training on fold {fold + 1}...")

    # Clone the pre-trained model for each fold
    model = clone_model(pretrained_model)
    model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

    # Extract the training and validation sets for this fold
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]


    # Train the model on the current fold
    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=3)

    # Evaluate the model on the validation set for this fold
    scores = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])

# Print average and standard deviation of performance across all folds
average_accuracy = np.mean(acc_per_fold)
std_dev_accuracy = np.std(acc_per_fold)

average_loss = np.mean(loss_per_fold)
std_dev_loss = np.std(loss_per_fold)

print("\nAverage Accuracy: {:.2f}% (+/- {:.2f}%)".format(average_accuracy, std_dev_accuracy))
print("Average Loss: {:.4f} (+/- {:.4f})".format(average_loss, std_dev_loss))